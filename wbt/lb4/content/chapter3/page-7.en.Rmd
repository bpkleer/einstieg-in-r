---
title: "Assumption 6: Influential Cases"  # Page title
weight: 307  # Custom weight 
menuTitle: "Influential Cases" # Short title if the title is too long
tags: ["regression diagnostics", "influential cases"]  # Tags; Brief description of what happens on the page
---
```{r setup, include = FALSE}
knitr::opts_chunk$set(
  echo = TRUE,   # Code wird angezeigt
  eval = FALSE,  # Code wird nicht ausgef端hrt
  message = FALSE, # Messages werden nicht angezeigt
  warnings = FALSE # warnings werden nicht angezeigt
)

library("modelsummary")

pss <- readRDS("./data/pss.rds")

olsModel2 <- lm(
  stfdem ~ 1 + stfeco + trstlgl,   
  data = pss
)      
```

{{% buttonShare href=\"https\:\/\/gitlab.com\/bpkleer\/einstieg-in-statistik\/issues\/new?issue[title]=\" icon=\"fas fa-bug\" %}} {{% /buttonShare %}}  

{{% buttonShare href=\"mailto\:\" icon=\"fas fa-paper-plane\" %}} {{% /buttonShare %}}

{{% buttonShare href=\"https\:\/\/t.me\/share\/url?url=\" icon=\"fab fa-telegram\" %}} {{% /buttonShare %}}

{{% buttonShare href=\"https\:\/\/api.whatsapp.com\/send?text=\" icon=\"fab fa-whatsapp\" %}} {{% /buttonShare %}}

{{% buttonShare href=\"https\:\/\/twitter.com\/share?url=\" icon=\"fab fa-x-twitter\" %}} {{% /buttonShare %}}

{{% buttonShare href=\"https\:\/\/www.facebook.com\/sharer\/sharer.php?u=\" icon=\"fab fa-facebook\" %}} {{% /buttonShare %}}

{{% button href=\"https\:\/\/bmc.link\/bpkleerw\" icon=\"fa-solid fa-beer-mug-empty\" %}} {{% /button %}}

To check for influential cases, we again use the residuals. Two values are relevant here: **Leverage** and **Cook's D(istance)**. Since **Cook's D(istance)** is partly based on **leverage**, we focus directly on **Cook's D**.

{{% tabs %}}
{{% tab name=\"Leverage\" %}}

- Is the partial derivative of the estimated $i$-th value of the dependent variable $\hat{y_i}$ with respect to the measured $i$-th value of the dependent variable $y_i$ 

- The degree to which the $i$-th measured value influences the $i$-th fitted value

- "Unusualness" of the $x$'s

- $h_{ii} = \frac{\partial \hat{y_i}}{\partial y_i}$

- Case is considered influential if $h_{ii} > .2$
{{% /tab %}}
{{% tab name=\"Cook's D\" %}}

- Is the distance of the change that a estimated value moves within the confidence ellipse that assumes the region of plausible values for the parameter

- Measures the influence (how strongly a case changes the regression model)

- $D_i = \frac{ \sum_{j=1}^n (\hat{y}_j - \hat{y}_{j(i)})}{p * s^2}$, where $p$ is the number of parameters

- Classified as influential if $D_i > \bar{D} *3$
{{% /tab %}}
{{% /tabs %}}

First, we can simply plot **Cook's D**. To do this, use the `plot()` function again, specify the regression model, and set the value `4` in the second argument:
```{r cooksd, eval=TRUE}
plot(
  olsModel2,
  4
)
```

**Thresholds**:

$D_i < 1$: no single observation

$\frac{4}{4890} \approx 0.0008179959$: many cases

$\bar{D} * 3 \approx 0.0005895857$: many cases

We can also directly plot **Cook's D** against the **leverage** and draw lines for the thresholds:
```{r eval=FALSE}
plot(
  olsModel2, 
  6
)

abline(
  h = 3 * mean(cooks.distance(olsModel2), na.rm = TRUE), 
  col = "chartreuse4", 
  lty = 6,  # Grenze Cook's D
  lwd = 2
)

abline(
  v = 2 * (2 + 1) / 2393, 
  col = "chartreuse4", 
  lty = 6,  # Grenze leverage
  lwd = 2
) 
```

To check if these theoretically potentially influential cases are indeed influential, we re-estimate the model excluding these observations.

For this, we need a dataset that has the same length as our model estimation:
```{r rerun, eval=TRUE}
nobs(olsModel2)

ccol <- c(
  "stfdem",
  "stfeco",
  "trstlgl"
)   

pss2 <- pss[complete.cases(pss[,ccol]),]

dim(pss2)
```

Now we can add the values of **leverage** (*hat values*) and **Cook's D** to the dataset.

```{r hats, eval=TRUE}
pss2$hats <- hatvalues(olsModel2)    

pss2$cooksd <- cooks.distance(olsModel2)  
```

Everything that follows now requires a deeper understanding, especially of loops and *if* conditions from [Learning Block 2](https://lehre.bpkleer.de/statsplus/lb2/chapter3/). **But** as mentioned, this chapter is extra and will not be part of the mandatory lab exercise.

To exclude the potentially influential cases, we need a dummy variable that indicates whether the case is influential according to the criteria ($1$) or not ($0): In the loop, we first check if the respective case value is above the threshold. If yes, the variable `leverage` is assigned the value 1 for leverage or `influence` for Cook's D.
```{r loop, eval=TRUE}
for (i in 1:dim(pss2)[1]) {   
if (pss2$hats[i] > (2*(2+1)/dim(pss2)[1])) {
   pss2$leverage[i] = 1
} else {
   pss2$leverage[i] = 0
}
}

for (i in 1:dim(pss2)[1]) {
   if (pss2$cooksd[i] > (3*mean(pss2$cooksd))) {
      pss2$influence[i] = 1
   } else {
      pss2$influence[i] = 0
   }
}
```

Now we can recalculate the model with the reduced dataset: Here we limit the dataset to the cases that have a value of `0` in the newly created variables `leverage` and `influence`, meaning they are not influential.
```{r model-rerun, eval=TRUE}
olsModel2Re <- lm(
  stfdem ~ 1 + stfeco + trstlgl,  
  data = pss2[pss2$leverage==0 & pss2$influence==0,]
)
```

To find out how many cases we have excluded, you can perform the following simple subtraction:
```{r diff, eval=TRUE}
nobs(olsModel2Re) - nobs(olsModel2)
```

So, 736 cases were excluded. Quite a lot!

Let's look at the result of the redefined model using `summary()`:
**Redefinition Model**
```{r model-check, eval=TRUE}
summary(olsModel2Re)
```
But does this now differ from the old model? To check this, we simply use the `modelsummary()` function we have already learned. In a table, we can compare the values better. It is important to check whether the standard errors or the coefficients change.
```{r model-check2, eval=TRUE}
modelsummary(
  list(
    olsModel2, 
    olsModel2Re
  ),
  stars = TRUE
)
```
$\Rightarrow$ You can see that the coefficient for the *intercept* as well as for `stfeco` changes. The *intercept* becomes smaller, while the effect of `stfdem` becomes larger. However, the standard errors (values in parentheses) do not change.

If we notice significant changes, the filtered cases need to be inspected more closely. We cannot simply exclude cases based on mathematical conditions.

To find out which cases are involved, you can program a loop again:
```{r extract-influentials, eval=TRUE}
levInfluential <- c()
cookInfluential <- c()
allInfluential <- c()

# nur f端r leverage
for (i in 1:dim(pss2)[1]) {
  if (pss2$leverage[i] == 1) {
    levInfluential <- c(levInfluential, pss2$idno[i])
  }
}

# nur f端r Cook's D
for (i in 1:dim(pss2)[1]) {
  if (pss2$influence[i] == 1) {
    cookInfluential <- c(cookInfluential, pss2$idno[i])
  }
}

# f端r beides
for (i in 1:dim(pss2)[1]) {
  if (pss2$leverage[i] == 1 & pss2$influence[i] == 1) {
    allInfluential <- c(allInfluential, pss2$idno[i])
  }
}
```

As a result, you will get three vectors, each containing the influential cases:
```{r result-influential, eval=TRUE}
levInfluential

cookInfluential

allInfluential
```

Then you could take a closer look at the value pairs on the variables in the regression for each case!

That's it for regression diagnostics. In these six steps, you can ensure that your regression calculation is not biased due to a violation of assumptions.

Please paste the Markdown content you would like me to translate.
