---
title: "Calculating Correlations"  # Page title
weight: 104  # Custom weight 
menuTitle: "Calculating Correlations" # Short title if the title is too long
tags: ["correlations"]  # Tags describing the content of the page
---
```{r setup, include = FALSE}
knitr::opts_chunk$set(
  echo = TRUE,   # Code wird angezeigt
  eval = FALSE,  # Code wird nicht ausgef√ºhrt
  message = FALSE, # Messages werden nicht angezeigt
  warnings = FALSE # warnings werden nicht angezeigt
)

xfun::pkg_load2(
                 c("htmltools", "mime")
               )
library("xaringanExtra")
library("psych")
pss <- readRDS("./data/pss.rds")
```

{{% buttonShare href=\"https\:\/\/gitlab.com\/bpkleer\/einstieg-in-statistik\/issues\/new?issue[title]=\" icon=\"fas fa-bug\" %}} {{% /buttonShare %}} 

{{% buttonShare href=\"mailto\:\" icon=\"fas fa-paper-plane\" %}} {{% /buttonShare %}}

{{% buttonShare href=\"https\:\/\/t.me\/share\/url?url=\" icon=\"fab fa-telegram\" %}} {{% /buttonShare %}}

{{% buttonShare href=\"https\:\/\/api.whatsapp.com\/send?text=\" icon=\"fab fa-whatsapp\" %}} {{% /buttonShare %}}

{{% buttonShare href=\"https\:\/\/twitter.com\/share?url=\" icon=\"fab fa-x-twitter\" %}} {{% /buttonShare %}}

{{% buttonShare href=\"https\:\/\/www.facebook.com\/sharer\/sharer.php?u=\" icon=\"fab fa-facebook\" %}} {{% /buttonShare %}}

{{% button href=\"https\:\/\/bmc.link\/bpkleerw\" icon=\"fa-solid fa-beer-mug-empty\" %}} {{% /button %}}

If you want to interpret not only the strength of the relationship but also the direction of the relationship, you can calculate the correlation.

This learning block introduces two correlation measures:

- **Pearson's r**

- **Spearman's $\rho$**

For the calculation of **Pearson's r**, the following conditions must be met:

- (pseudo-)metric variables

- linear (monotonic) relationship

- equal variance

- (bivariate normal distribution)

For the calculation of **Spearman's $\rho$**, only the following conditions need to be met:

- (at least) ordinal variables

- monotonic relationship

## Linear and Non-linear Relationships
The figure presents four examples that would all yield nearly identical statistical measures (*Anscombe's quartet*).

![Linearity and Non-linearity](../img/linearity.png)

Panel A shows a linear and monotonic relationship between two variables. In this case, calculating **Pearson's r** would be appropriate. Panel B, although showing a monotonic relationship, is not linear. In this case, **Spearman's $\rho$** can be calculated. Panel C demonstrates how an outlier can alter the relationship structure, leading both correlation measures to provide a biased value. Panel D shows a non-linear and non-monotonic relationship. 

It becomes clear here that before calculating measures, graphical analysis is helpful or necessary!

## Example Correlation
Now you should calculate the correlation between *Trust in Parliament* (`trstprl`) and *Trust in Politicians* (`trstplt`) from the **PSS**.

Both variables are pseudo-metric variables, so you should calculate **Pearson's r**.

To do this, you need to test the assumptions of **Pearson's r**:

- Sample of paired values $\checkmark$

- both variables are metric $\checkmark$

- Relationship between variables is linear

### Testing the Assumption
You can easily check this by creating a scatter plot. You can use the *base* function `plot()` for this. We will learn the more powerful graphics *library* `ggplot2` in the last learning block.
```{r corr-plot1, eval=TRUE}
plot(
  pss$trstprl, 
  pss$trstplt
)
```

Since the data is only pseudo-metric and many data points (integer) overlap, you can't see much from the plot.

**Solution**: Use the `jitter()` function to spread the points more widely:
```{r eval=TRUE}
plot(
  jitter(
    pss$trstprl, 
    3
  ) ~ 
    jitter(
      pss$trstplt, 
      3
    )
)
```
This graphical analysis takes some getting used to: If you don't see clear patterns like in the Anscombe quartet, you can assume a linear relationship.

In conclusion, we can state that the conditions are met:

- Sample of paired values $\checkmark$

- both variables are metric $\checkmark$

- Relationship between variables is linear $\checkmark$

$\Rightarrow$ You can now calculate **Pearson's r**!

### Calculating the Coefficient

To calculate the correlation coefficient, you use the `cor()` function (for both **Pearson's r** and **Spearman's** $\rho$). First, you need to name the two variables in the function. Then you should choose the correlation coefficient and finally select how to deal with `NA's` in the variables. Here, you delete any row that has an `NA` value in either of the two variables.
```{r corrcalc, eval=TRUE}
cor(
  pss$trstprl, 
  pss$trstplt, 
  method = "pearson",  # alternativ hier "spearman"  
  use = "complete.obs"
)      
```

The output shows a correlation coefficient of $r \approx 0.232. In this output, there is no p-value included, and you cannot make a statement about significance.

### Calculating the coefficient with the **library** `psych`
With the *library* `psych`, you can use the function `corr.test()`, which also provides the significance test:

```{r cor-psych, eval=FALSE}
install.packages("psych")
library("psych")
```
```{r cor-psych1, eval=TRUE}
corr.test(
  pss$trstprl, 
  pss$trstplt,
  method = "pearson",      
  use = "complete.obs"
)     
```

This test generates three matrices (correlation matrix, sample size matrix, p-value matrix) that you can later use for visualization.

The additional information needed for the p-value is found at the last position. Here, a p-value of $0$ is present.

{{% expand "What does the p-value at this point mean?" %}}
The p-value is less than $0.05$, indicating a statistically significant relationship between trust in parliament and trust in politicians. However, this relationship is weak ($r=0.23$).
{{% /expand %}}

### Calculating multiple correlations
With both functions, you can calculate not only the correlation between two variables but also specify more than two variables directly. It will then calculate the correlation pairwise between all variables. To do this, use the function `c()` to specify between which variables you want to get pairwise correlation values:
```{r cor-group1, eval=TRUE}
cor(
  pss[
    ,
    c(
      "trstprl", 
      "trstplt",
      "trstprt",
      "trstlgl"
    )
  ],
  method = "pearson",
  use = "complete.obs"
)  

corr.test(
  pss[
    ,
    c(
      "trstprl",
      "trstplt",
      "trstprt",
      "trstlgl"
    )
  ],
  method = "pearson",
  use = "complete.obs"
)  
```

Both outputs show a correlation matrix. The variable names are given in the columns and rows. The diagonal is always $1$ because the relationship between the variable and itself is $1$ (perfect!).

{{% expand "Example! What is the correlation coefficient for the relationship between trstplt and trstprt?" %}}
The correlation coefficient is $r = 0.40$, indicating a moderate relationship between the two variables.
{{% /expand %}}

How do we graphically represent correlations now?
